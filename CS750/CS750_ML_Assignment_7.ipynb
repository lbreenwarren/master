{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "To do this assignment, save a copy to your own Google drive by going to:\n",
        "> File --> Save a copy in Drive\n",
        "\n",
        "Before working on this notebook, please disable inline AI suggestions by going to:\n",
        "\n",
        "> Settings (top right gear icon) --> AI assistance --> Show AI-powered inline completions (uncheck)\n",
        "\n",
        "As stated in the syllabus, you're welcome to use AI as an aid, but your answers should be your own.\n",
        "\n",
        "Submission instructions:\n",
        "\n",
        "-   **Due**: Tuesday 10/21 at 11:59PM AOE\n",
        "-   **Submission**: Turn in both a printed **PDF** and the **editable share link** on [MyCourses](http://mycourses.unh.edu).\n",
        "\n",
        "Remember that to make the share link editable, you must **convert the notebook to \"anyone with a link can edit\"** before copying and pasting the link.\n"
      ],
      "metadata": {
        "id": "mqeoElZiGk5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1\n",
        "\n",
        "You will derive the formula used to compute the solution to *ridge regression*. The objective in ridge regression is:\n",
        "$$ f(\\beta) = \\lVert y - A \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_2^2 $$\n",
        "Here, $\\beta$ is the vector of coefficients that we want to optimize, $A$ is the design matrix, $y$ is the target, and $\\lambda$ is the regularization coefficient. The notation $\\lVert \\cdot \\rVert_2$ represents the Euclidean (or $L_2$) norm.\n",
        "\n",
        "Our goal is to find $\\beta$ that solves:\n",
        "$$ \\min_\\beta f(\\beta) $$\n",
        "Follow the next steps to compute it.\n"
      ],
      "metadata": {
        "id": "nDxbDF7p99ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xtGaZZb6BMbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1.1\n",
        "\n",
        "Express the ridge regression objective $f(\\beta)$ in terms of linear and quadratic terms. Recall that $\\lVert \\beta \\rVert_2^2 = \\beta^T \\beta$. The result should be similar to the objective function of linear regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "gIX2o9cW-DMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f(β)=∥y−Aβ∥^2_2+λ∥β∥^2_2$\n",
        "\n",
        "$= (y − Aβ)^T (y − Aβ) + λ · β^Tβ$\n",
        "\n",
        "$= y^Ty − 2y^T Aβ + β^T(A^T A + λI)β$"
      ],
      "metadata": {
        "id": "a_mufdlQBVsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1.2\n",
        "\n",
        "Derive the gradient: $\\nabla_\\beta f(\\beta)$ using the linear and quadratic terms above.\n",
        "\n"
      ],
      "metadata": {
        "id": "oKx3-FZa-Hwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\nabla_\\beta f(\\beta) = \\nabla_\\beta(y^Ty − 2y^T Aβ + β^T(A^T X + λI)β)$\n",
        "\n",
        "$=\\nabla_\\beta(-2y^T Aβ + β^T(A^T X + λI)β)$\n",
        "\n",
        "$=-2A^T y + (A^A X + λI)β$"
      ],
      "metadata": {
        "id": "ipstCNlVCBF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1.3\n",
        "\n",
        "Since $f$ is convex, its minimal value is attained when\n",
        "$$ \\nabla_\\beta f(\\beta) = 0$$\n",
        "Derive the expression for the $\\beta$ that satisfies the inequality above. You can adapt the derivation of the similar formula for linear regression from the slides.\n"
      ],
      "metadata": {
        "id": "t7k3PWqrwBXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$-2A^T y + (A^T X + λI)β = 0$\n",
        "\n",
        "$(A^T X + λI)β = 2A^T y$\n",
        "\n",
        "$β = (A^T X + λI)^{-1} + 2A^T y$\n"
      ],
      "metadata": {
        "id": "OnYSOl7aC_Bm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1.4\n",
        "\n",
        "Implement the algorithm for computing $\\beta$ and use it on a small dataset of your choice. Do not forget about the intercept.\n",
        "\n",
        "**Hint**: [numpy.linalg.inv()](https://numpy.org/doc/2.3/reference/generated/numpy.linalg.inv.html) will come in handy here\n"
      ],
      "metadata": {
        "id": "bcTUBVCCwGZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.array([[1.0, 2.0],\n",
        "              [2.0, 0.0],\n",
        "              [3.0, 1.0],\n",
        "              [4.0, 3.0]])\n",
        "X0 = np.hstack([np.ones((4, 1)), X]) #add a column of 1's for intercept\n",
        "y = np.array([1.0, 2.5, 3.0, 5.0])\n",
        "y0 = np.asarray(y, dtype=float).reshape(-1, 1)\n",
        "n,p = X0.shape\n",
        "lamb = 1.0\n",
        "I = np.eye(p)\n",
        "I[0, 0] = 0 #dont penalize the intercept\n",
        "a = (X0.T @ X0) + (lamb*I)\n",
        "b = X0.T @ y0\n",
        "beta = np.linalg.solve(a, b)\n",
        "print(beta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arEDt4p6DO6A",
        "outputId": "c8cb8ee0-cd4a-4d3c-de9a-58975ac5c71a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.1875]\n",
            " [1.    ]\n",
            " [0.125 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1.5\n",
        "\n",
        "Compare your solution with [the Scikit-Learn implementation of ridge regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) (or another standard implementation) using a small example. Are the results the same? Why yes, or no?\n"
      ],
      "metadata": {
        "id": "3BYL0DlvwJSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "model = Ridge(alpha=1.0, fit_intercept=True)\n",
        "model.fit(X,y)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ05yxt8KNvi",
        "outputId": "83e69b1b-d4a1-4578-9669-41bf03c73cee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 0.18749999999999956\n",
            "Coefficients: [1.    0.125]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our results our very nearly identicle because I perform the same algorithm as scikit learn's implementation. At first I was getting different results because I was penalizing the intercept, but once I correctly cleared the (0,0) index of the identity matrix they lined up"
      ],
      "metadata": {
        "id": "eu-athVaMJiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2\n",
        "\n",
        "You will now derive the Bayesian connection to the lasso as discussed in Section 6.2.2. of ISL.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zatRgEd1-UZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2.1\n",
        "\n",
        "Suppose that $y_i = \\beta_0 + \\sum_{j=1}^p x_{ij} \\beta_j + \\epsilon_i$ where $\\epsilon_1,\\ldots,\\epsilon_n$ are independent and identically distributed from a normal distribution $\\mathcal{N}(0, 1)$. Write out the likelihood for the data as a function of values $\\beta$.\n"
      ],
      "metadata": {
        "id": "Z6NU2gjlsC-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$L(\\beta) = ℙ(y | \\beta) = \\frac{1}{\\sqrt{2π}}exp({-\\frac{1}{2}(y_i - βx_i)^2})$\n"
      ],
      "metadata": {
        "id": "QduXpna0C15P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2.2\n",
        "\n",
        "Assume that the prior for $\\beta: \\beta_1 , \\ldots , \\beta_p$ is that they are independent and identically distributed according to the *Laplace* distribution with mean zero and variance $c$. Write out the posterior for $\\beta$ in this setting using Bayes theorem.\n"
      ],
      "metadata": {
        "id": "C7dD-Rqksasx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By Bayes Theorem the Posterior is:\n",
        "\n",
        "$ℙ(\\beta | y) = \\frac{ℙ(y | \\beta)\n",
        "ℙ(β)}{ℙ(y)} ∝ ℙ(y|β)ℙ(y)$\n",
        "\n",
        "and the Laplace Prior for β values is:\n",
        "\n",
        "$ℙ(y) = g(β) = \\frac{1}{2}exp(\\frac{|β|}{1})$\n",
        "\n",
        "So:\n",
        "\n",
        "$ℙ(\\beta | y) = \\frac{1}{\\sqrt{2π}}exp({-\\frac{1}{2}(y_i -βx_i)^2})\\frac{g(β)}{ℙ(y)} ∝ ℙ(β| y)ℙ(y)$"
      ],
      "metadata": {
        "id": "YvgZOHYVB-1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2.3\n",
        "\n",
        "Argue that the lasso estimate is the value of $\\beta$ with maximal probability under this posterior distribution. Compute $\\log$ of the probability in order to make this point. *Hint*: The denominator (= the probability of data) can be ignored when computing the maximum probability.\n"
      ],
      "metadata": {
        "id": "qgFVfAw5sbwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$logℙ(\\beta | y) = logℙ(y | \\beta_) + logℙ(y) + C$\n",
        "\n",
        "$= ∑^n_{i=1}log(\\frac{1}{\\sqrt{2π}}exp({-\\frac{1}{2}(y_i - βx_i)^2})) + ∑^p_{j=1}log(\\frac{1}{2}exp(|β|)$\n",
        "\n",
        "$= -\\frac{1}{2}||y_i- Xβ||^2_2 + ∑_{j=1}^p |β_j| + C$"
      ],
      "metadata": {
        "id": "EGdf4b2eRjkK"
      }
    }
  ]
}